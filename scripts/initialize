#!/usr/bin/env bash

function ensure_pool {
  local pool_name=$1
  local pool_description=$2
  local pool_info=$(airflow pool -g ${pool_name})

  echo "Ensuring pool $pool_name exists"
  if [[ $pool_info =~ .*ERROR.* ]]; then
    echo "Pool $pool_name does not exist, creating"
    airflow pool -s $pool_name 1 $pool_description
  fi
}

echo "Initialize airflow database"
airflow initdb

echo "Initialize standard variables"
airflow variables --set DATAFLOW_WRAPPER_LOG_PATH "${AIRFLOW_HOME}/log_wrapper"
airflow variables --set DATAFLOW_WRAPPER_STUB "${AIRFLOW_HOME}/utils/log_wrapper.py"
airflow variables --set DOCKER_RUN "gcloud docker -- run"
airflow variables --set EVENTS_DATASET $EVENTS_DATASET
airflow variables --set PIPELINE_BUCKET $PIPELINE_BUCKET
airflow variables --set PIPELINE_DATASET $PIPELINE_DATASET
airflow variables --set PIPELINE_START_DATE $PIPELINE_START_DATE
airflow variables --set PROJECT_ID $PROJECT_ID
airflow variables --set TEMP_BUCKET $TEMP_BUCKET

echo "Initialize worker pools"
ensure_pool dataflow 1 "Google Cloud Dataflow jobs"
ensure_pool bigquery 1 "Google Bigquery jobs"
ensure_pool local-cpu 1 "Local CPU"

echo "Update default GCP connection"
airflow connections --delete  \
  --conn_id=google_cloud_default

airflow connections --add   \
  --conn_id=google_cloud_default \
  --conn_type=google_cloud_platform  \
  --conn_extra={\"extra__google_cloud_platform__project\":\"${PROJECT_ID}\"}
